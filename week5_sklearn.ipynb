{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NX-414: Brain-like computation and intelligence\n",
    "##### TA: Alessandro Marin Vargas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Week 5 - Mini project (Predicting neural activity)\n",
    "\n",
    "The objectives of the mini project are:\n",
    "- Learn how to predict neural activity using linear regression from images and from neural network layers.\n",
    "- Quantify the goodness of the model\n",
    "- Compare the results across the network layers and between trained/random neural network\n",
    "- Predict the neural activity using a neural network in a data-driven approach\n",
    "\n",
    "Specifically, here you will use the data from the following [paper](https://www.jneurosci.org/content/jneuro/35/39/13402.full.pdf). The behavioral experiment consisted in showing to non-human primates some images while recording the neural activity with multielectrode arrays from the inferior temporal (IT) cortex. In the data we provided you, the neural activity and the images are already pre-processed and you will have available the images and the corresponding average firing rate (between 70 and 170 ms) per each neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gdown h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell only the first time you run the notebook\n",
    "run_this_cell = False\n",
    "if run_this_cell:\n",
    "    import gdown\n",
    "    url = \"https://drive.google.com/file/d/1s6caFNRpyR9m7ZM6XEv_e8mcXT3_PnHS/view?usp=share_link\"\n",
    "    output = \"data/IT_data.h5\"\n",
    "    gdown.download(url, output, quiet=False, fuzzy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import useful libraries\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = 'data' \n",
    "stimulus_train, stimulus_val, stimulus_test, objects_train, objects_val, objects_test, spikes_train, spikes_val = load_it_data(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stimulus, n_channels, img_size, _ = stimulus_train.shape\n",
    "_, n_neurons = spikes_train.shape\n",
    "print('The train dataset contains {} stimuli and {} IT neurons'.format(n_stimulus,n_neurons))\n",
    "print('Each stimulus have {} channgels (RGB)'.format(n_channels))\n",
    "print('The size of the image is {}x{}'.format(img_size,img_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now visualize the neuron response to the different stimuli\n",
    "neuron_idx = 0\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Average firing rate for neuron {} (70-170 ms)'.format(neuron_idx))\n",
    "plt.plot(spikes_train[:,neuron_idx])\n",
    "# note that each point represents average respose of the neuron over 100 ms to a different stimulus\n",
    "# in fact there are 2592 peaks in the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Predict the neural activity from pixels\n",
    "\n",
    "##### Develop a linear regression model that predict the neural activity from pixels.\n",
    "You can try out different types of linear regression (ridge, least-square regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ridgecross import *\n",
    "#shall we include the class of the objects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform objects to 8 classes and to int\n",
    "objects_train_8 = transform_classes_to_int(transform_to_8_classes(objects_train))\n",
    "objects_val_8 = transform_classes_to_int(transform_to_8_classes(objects_val))\n",
    "\n",
    "# Flatten stimulus data\n",
    "stimulus_train_res = stimulus_train.reshape((2592, -1))\n",
    "stimulus_val_res = stimulus_val.reshape((288, -1))\n",
    "\n",
    "# concatenate the 2 arrays\n",
    "X_train = np.concatenate((stimulus_train_res, np.array(objects_train_8).reshape(-1,1)), axis=1)\n",
    "X_val = np.concatenate((stimulus_val_res, np.array(objects_val_8).reshape(-1,1)), axis=1)\n",
    "\n",
    "print('Shape of X_train: ', X_train.shape)\n",
    "print('Shape of X_val: ', X_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import explained_variance_score\n",
    "\n",
    "# one-hot encode the class labels for the training set\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "class_labels_one_hot = encoder.fit_transform(X_train[:,-1].reshape(-1, 1))\n",
    "\n",
    "# one-hot encode the class labels for the validation set\n",
    "class_labels_one_hot_val = encoder.transform(X_val[:,-1].reshape(-1, 1))\n",
    "\n",
    "# concatenate the one-hot encoded labels to the feature matrix\n",
    "X_train_augmented = np.hstack((X_train[:,:-1], class_labels_one_hot))\n",
    "X_val_augmented = np.hstack((X_val[:,:-1], class_labels_one_hot_val))\n",
    "\n",
    "# fit ridge regression model to augmented data\n",
    "ridge = Ridge(alpha=10, fit_intercept=True)\n",
    "ridge.fit(X_train_augmented, spikes_train)\n",
    "\n",
    "# make predictions using validation set\n",
    "y_pred = ridge.predict(X_val_augmented)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate your prediction (Check both the correlation and explained variance for each neuron). Plot the distribution for the explained variance across neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate explained variance score for predictions\n",
    "evs = explained_variance_score(spikes_val, y_pred)\n",
    "\n",
    "# compute the explained variance per neuron\n",
    "ev_per_neuron = 1 - np.var(y_pred - spikes_val, axis=0) / np.var(spikes_val, axis=0)\n",
    "# print(evs, np.mean(ev_per_neuron)) # they are the same as expected\n",
    "\n",
    "# print histogram of explained variance per neuron\n",
    "plt.hist(ev_per_neuron, bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predicting from pixels is very hard and the model is likely to overfit. An image is very high-dimensional, try to retain the corresponding 1000 PCs and use them to predict the neural activity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform PCA\n",
    "\n",
    "n_comp = 1000\n",
    "\n",
    "pca = PCA(n_components=n_comp)\n",
    "pca.fit(stimulus_train_res)\n",
    "pca_filters = pca.components_\n",
    "pca_features_train = pca.transform(stimulus_train_res)\n",
    "print(pca_features_train.shape)\n",
    "\n",
    "# Take 1000 PCs for the validation set as well\n",
    "pca_features_val = pca.transform(stimulus_val_res)\n",
    "print(pca_features_val.shape)\n",
    "\n",
    "# Now join them with the object_8 filters\n",
    "\n",
    "pca_feature_matrix_8 = np.concatenate((pca_features_train, np.array(objects_train_8).reshape(-1,1)), axis=1)\n",
    "print(pca_feature_matrix_8.shape)\n",
    "\n",
    "# same for validation set\n",
    "pca_feature_matrix_8_val = np.concatenate((pca_features_val, np.array(objects_val_8).reshape(-1,1)), axis=1)\n",
    "print(pca_feature_matrix_8_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the one-hot encoded labels to the feature matrix\n",
    "pca_feature_matrix_8_augmented = np.hstack((pca_feature_matrix_8[:,:-1], class_labels_one_hot))\n",
    "pca_feature_matrix_8_val_augmented = np.hstack((pca_feature_matrix_8_val[:,:-1], class_labels_one_hot_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import explained_variance_score\n",
    "\n",
    "# fit ridge regression model with arbitrary lambda: 10\n",
    "num_classes = 8\n",
    "lamb = 10\n",
    "ridge = Ridge(alpha=lamb, fit_intercept=True)\n",
    "ridge.fit(pca_feature_matrix_8_augmented, spikes_train)\n",
    "\n",
    "# make predictions using validation set\n",
    "y_pred = ridge.predict(pca_feature_matrix_8_val_augmented)\n",
    "\n",
    "# compute the explained variance for each neuron\n",
    "ev_per_neuron = 1 - np.var(y_pred - spikes_val, axis=0) / np.var(spikes_val, axis=0)\n",
    "# compute overall explained variance\n",
    "ev = explained_variance_score(spikes_val, y_pred)\n",
    "print('Overall explained variance:', ev)\n",
    "\n",
    "# compute the correlation for each neuron\n",
    "corr_per_neuron = np.array([np.corrcoef(y_pred[:, i], spikes_val[:, i], rowvar=False)[0, 1] for i in range(y_pred.shape[1])])\n",
    "# compute overall correlation\n",
    "corr = np.mean(corr_per_neuron)\n",
    "print('Overall correlation:', corr)\n",
    "\n",
    "# create subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# print histogram of explained variance per neuron\n",
    "ax = axs[0]\n",
    "ax.hist(ev_per_neuron, bins=20)\n",
    "ax.set_title('Explained variance per neuron')\n",
    "\n",
    "# print histogram of correlation per neuron\n",
    "ax = axs[1]\n",
    "ax.hist(corr_per_neuron, bins=20)\n",
    "ax.set_title('Correlation per neuron')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Can we improve the prediction? Using the ridge regression, find the best parameter with cross-fold validation (remember to split the data keeping the same distribution of classes between the train and validation set). Does it get better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm \n",
    "\n",
    "# number of classes\n",
    "num_classes = 8\n",
    "\n",
    "# concatenate the one-hot encoded labels to the feature matrix\n",
    "X_augmented = pca_feature_matrix_8_augmented\n",
    "\n",
    "# define the splitter object\n",
    "splitter = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# initialize lists to store the results\n",
    "best_lambda_per_neuron = []\n",
    "best_ev_per_neuron = []\n",
    "\n",
    "# create candidate lambda values\n",
    "lambdas = [0.1, 1, 10, 50, 100, 200, 500, 750, 1000]\n",
    "\n",
    "# loop over different neurons\n",
    "for neuron in tqdm(range(spikes_train.shape[1])):\n",
    "    best_ev = -5\n",
    "    best_lamb = 0\n",
    "\n",
    "    # loop over different lambda values\n",
    "    \n",
    "    for lamb in lambdas:\n",
    "        evs = []\n",
    "        # split into training and validation sets\n",
    "        for (train_index, val_index) in splitter.split(pca_feature_matrix_8[:,:-1], pca_feature_matrix_8[:,-1]):\n",
    "            # split the data into training and validation sets\n",
    "            X_train_augmented, X_val_augmented = X_augmented[train_index,:], X_augmented[val_index,:]\n",
    "            y_train, y_val = spikes_train[train_index, neuron], spikes_train[val_index, neuron]\n",
    "                \n",
    "            # fit ridge regression model to X_train after one-hot encoding the class labels\n",
    "            ridge = Ridge(alpha=lamb, fit_intercept=True)\n",
    "            ridge.fit(X_train_augmented, y_train)\n",
    "            \n",
    "            # make predictions using validation set\n",
    "            y_pred = ridge.predict(X_val_augmented)\n",
    "            \n",
    "            # compute the ev for each neuron on the val set \n",
    "            ev_per_neuron = explained_variance_score(y_val, y_pred)\n",
    "            evs.append(ev_per_neuron)\n",
    "            #print('Explained variance for neuron {0} and lambda = {1}: {2} '.format(neuron, lamb, ev_per_neuron))\n",
    "        ev = np.mean(evs)\n",
    "        if ev > best_ev:\n",
    "            best_ev = ev\n",
    "            best_lamb = lamb\n",
    "    \n",
    "    best_ev_per_neuron.append(best_ev)\n",
    "    best_lambda_per_neuron.append(best_lamb)\n",
    "        \n",
    "    #print('Explained variance for neuron {0} and lambda = {1}: {2} '.format(neuron, best_lamb, best_ev))\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now use the best lambda for each neuron to fit the model on the entire training set\n",
    "# and evaluate it on the validation set\n",
    "\n",
    "num_classes = 8\n",
    "ev_per_neuron = []\n",
    "corr_per_neuron = []\n",
    "for neuron in tqdm(range(spikes_train.shape[1])):\n",
    "    lamb = best_lambda_per_neuron[neuron]\n",
    "    ridge = Ridge(alpha=lamb, fit_intercept=True)\n",
    "    ridge.fit(pca_feature_matrix_8_augmented, spikes_train[:, neuron])\n",
    "\n",
    "    # make predictions using validation set\n",
    "    y_pred = ridge.predict(pca_feature_matrix_8_val_augmented)\n",
    "\n",
    "    # compute explained variance\n",
    "    ev = explained_variance_score(spikes_val[:,neuron], y_pred)\n",
    "    ev_per_neuron.append(ev)\n",
    "\n",
    "    # compute the correlation for each neuron\n",
    "    corr = np.corrcoef(y_pred, spikes_val[:, neuron], rowvar=False)[0, 1]\n",
    "    #corr = np.mean(corr)\n",
    "    corr_per_neuron.append(corr)\n",
    "\n",
    "# create subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# print histogram of explained variance per neuron\n",
    "ax = axs[0]\n",
    "ax.hist(ev_per_neuron, bins=20)\n",
    "ax.set_title('Explained variance per neuron')\n",
    "\n",
    "# print histogram of correlation per neuron\n",
    "ax = axs[1]\n",
    "ax.hist(corr_per_neuron, bins=20)\n",
    "ax.set_title('Correlation per neuron')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# print the average explained variance and correlation\n",
    "print('Average explained variance:', np.mean(ev_per_neuron))\n",
    "print('Average correlation:', np.mean(corr_per_neuron))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Predict the neural activity with the task-driven modeling approach\n",
    "\n",
    "As you have seen in the class, the underlying hypothesis of task-driven modeling is that training the network to perform a relevant behavioral task makes the network to develop representations that resemble the ones of the biological brain. Let's test this hypothesis by loading a pre-trained ResNet50 model and use the activations of each layer to predict the neural activity. Follow these steps:\n",
    "\n",
    "- Give as input to the network the stimuli and extract the corresponding activations of the following layers ['conv1','layer1','layer2','layer3','layer4','avgpool']\n",
    "- Compute the 1000 PCs for each layer activation. (Careful that you don't want to store all activations together at the same time because it won't fit in the memory. Therefore, compute the activations and corresponding PCs for each layer and store only the computed PCs).\n",
    "- Use the PCs of each layer to predict the neural activity using the linear regression models you developed before.\n",
    "- Compute the goodness of fit using the correlation and explained variance metrics. Do you predict the neural activity better than before?\n",
    "- Plot the distribution of explained variance with respect to the layer of the network (order them based on the depth). How does the neural activity changes across the model layers, can you make some statements about it?\n",
    "- Compare the predictions that you obtained using one layer of the pretrained model and the one obtained using the same layer but from a randomly initialized model. Which network can better predict the neural activity and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "weights = ResNet50_Weights.IMAGENET1K_V2\n",
    "model = resnet50(weights=weights)\n",
    "\n",
    "n_comp = 1000\n",
    "pca = PCA(n_components=n_comp)\n",
    "def get_features(name):\n",
    "    def hook_fun(model, input, output):\n",
    "        #print(output.detach().reshape([2592,-1]).shape)\n",
    "        pca.fit(output.detach().reshape([2592,-1]))\n",
    "        features[name] = pca.transform(output.detach().reshape([2592,-1]))\n",
    "    return hook_fun\n",
    "\n",
    "# create a dictionary to store the features\n",
    "features = {}\n",
    "\n",
    "# get the first convolutional layer\n",
    "conv1 = model.conv1.register_forward_hook(get_features('conv1'))\n",
    "model(torch.tensor(stimulus_train))\n",
    "conv1.remove()\n",
    "\n",
    "# get layer1\n",
    "layer1 = model.layer1.register_forward_hook(get_features('layer1'))\n",
    "model(torch.tensor(stimulus_train))\n",
    "layer1.remove()\n",
    "\n",
    "# get layer2\n",
    "layer2 = model.layer2.register_forward_hook(get_features('layer2'))\n",
    "model(torch.tensor(stimulus_train))\n",
    "layer2.remove()\n",
    "\n",
    "# get layer3\n",
    "layer3 = model.layer3.register_forward_hook(get_features('layer3'))\n",
    "model(torch.tensor(stimulus_train))\n",
    "layer3.remove()\n",
    "\n",
    "# get layer4\n",
    "layer4 = model.layer4.register_forward_hook(get_features('layer4'))\n",
    "model(torch.tensor(stimulus_train))\n",
    "layer4.remove()\n",
    "\n",
    "# get avgpool\n",
    "avgpool = model.avgpool.register_forward_hook(get_features('avgpool'))\n",
    "model(torch.tensor(stimulus_train))\n",
    "avgpool.remove()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m104"
  },
  "kernelspec": {
   "display_name": "epfl-mathis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
